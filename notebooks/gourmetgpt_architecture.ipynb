{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71149fd6",
   "metadata": {},
   "source": [
    "# GourmetGPT: Recipe Generation Model Development\n",
    "\n",
    "This notebook implements, trains, evaluates, and exports the GourmetGPT model in accordance with the project constitution and specification.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Import Dependencies and Set Up Environment\n",
    "- Install and import required libraries (PyTorch, transformers, etc.)\n",
    "- Set random seeds for reproducibility\n",
    "- Configure Google Drive integration for artifact storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74571c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import required libraries\n",
    "!pip install torch transformers --quiet\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from google.colab import drive\n",
    "\n",
    "def set_seed(seed=123):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(123)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "print('Google Drive mounted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e01ee",
   "metadata": {},
   "source": [
    "## 2. Load and Validate Datasets\n",
    "- Load pretraining and fine-tuning datasets from Google Drive\n",
    "- Validate schema and completeness\n",
    "- Perform automated checks as per constitution.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dbf4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretraining dataset (.txt)\n",
    "pretrain_path = '/content/drive/MyDrive/GourmetGPT/Dataset/structured_recipes_pretrain.txt'\n",
    "with open(pretrain_path, 'r', encoding='utf-8') as f:\n",
    "    pretrain_data = f.read().split('[EOS]')\n",
    "pretrain_data = [r.strip() for r in pretrain_data if r.strip()]\n",
    "print(f'Loaded {len(pretrain_data)} pretraining recipes.')\n",
    "\n",
    "# Validate pretraining schema\n",
    "for i, recipe in enumerate(pretrain_data[:3]):\n",
    "    assert '[BOS]' in recipe and 'Title:' in recipe and 'Ingredients:' in recipe and 'Instructions:' in recipe, f\"Schema error in recipe {i}\"\n",
    "print('Pretraining dataset schema validated.')\n",
    "\n",
    "# Load fine-tuning dataset (.jsonl)\n",
    "import json\n",
    "finetune_path = '/content/drive/MyDrive/GourmetGPT/Dataset/structured_recipes_finetune.jsonl'\n",
    "finetune_data = []\n",
    "with open(finetune_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        finetune_data.append(obj)\n",
    "print(f'Loaded {len(finetune_data)} fine-tuning samples.')\n",
    "\n",
    "# Validate fine-tuning schema\n",
    "for i, sample in enumerate(finetune_data[:3]):\n",
    "    assert 'instruction' in sample and 'response' in sample, f\"Schema error in sample {i}\"\n",
    "    assert '[BOS]' in sample['response'] and 'Title:' in sample['response'] and 'Ingredients:' in sample['response'] and 'Instructions:' in sample['response'], f\"Response schema error in sample {i}\"\n",
    "print('Fine-tuning dataset schema validated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268a14b",
   "metadata": {},
   "source": [
    "## 3. Design and Implement GPT Architecture\n",
    "- Define the GPT model architecture using PyTorch\n",
    "- Configure tokenizer and model configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07996ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GPT model architecture (simplified, for demonstration)\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x, x)\n",
    "        return self.fc_out(x)\n",
    "\n",
    "# Example config\n",
    "vocab_size = 32000  # Placeholder, set after tokenizer training\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "model = GPTModel(vocab_size, embed_dim, num_heads, num_layers)\n",
    "print(model)\n",
    "\n",
    "# Tokenizer setup (placeholder)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8633aab",
   "metadata": {},
   "source": [
    "## 4. Train Model and Save Checkpoints\n",
    "- Train the model on the loaded datasets\n",
    "- Periodically save checkpoints to Google Drive\n",
    "- Log training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79469e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized training loop for A100 GPU with mixed precision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        return tokens['input_ids'].squeeze(0)\n",
    "train_dataset = RecipeDataset(pretrain_data, tokenizer)\n",
    "# Increase batch size for A100 (try 32, adjust if OOM)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "model.train()\n",
    "for epoch in range(1):  # Demo: 1 epoch\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(batch)\n",
    "            loss = nn.CrossEntropyLoss()(output.view(-1, vocab_size), batch.view(-1))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        # Save checkpoint every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            torch.save(model.state_dict(), '/content/drive/MyDrive/GourmetGPT/model_state_dict.pth')\n",
    "print('Training complete. Final checkpoint saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b2bf7",
   "metadata": {},
   "source": [
    "## 5. Export Model Artifacts\n",
    "- Export trained model state_dict, tokenizer, and config files as artifacts for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model artifacts\n",
    "model_path = '/content/drive/MyDrive/GourmetGPT/model_state_dict.pth'\n",
    "tokenizer_path = '/content/drive/MyDrive/GourmetGPT/tokenizer.json'\n",
    "config_path = '/content/drive/MyDrive/GourmetGPT/config.json'\n",
    "\n",
    "# Save model state_dict\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f'Model weights saved to {model_path}')\n",
    "\n",
    "# Save tokenizer\n",
    "if hasattr(tokenizer, 'save_pretrained'):\n",
    "    tokenizer.save_pretrained('/content/drive/MyDrive/GourmetGPT/')\n",
    "    print(f'Tokenizer saved to {tokenizer_path}')\n",
    "else:\n",
    "    import json\n",
    "    with open(tokenizer_path, 'w') as f:\n",
    "        json.dump(tokenizer.__dict__, f)\n",
    "    print(f'Tokenizer config saved to {tokenizer_path}')\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embed_dim': embed_dim,\n",
    "    'num_heads': num_heads,\n",
    "    'num_layers': num_layers\n",
    "}\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f)\n",
    "print(f'Model config saved to {config_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712aecc0",
   "metadata": {},
   "source": [
    "## 6. Unit Tests for Model and Data\n",
    "- Implement unit tests in notebook cells to verify model components and data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test: Model output shape\n",
    "sample_input = torch.randint(0, vocab_size, (2, 256))  # batch_size=2, seq_len=256\n",
    "output = model(sample_input)\n",
    "assert output.shape == (2, 256, vocab_size), f\"Unexpected output shape: {output.shape}\"\n",
    "print('Model output shape test passed.')\n",
    "\n",
    "# Unit test: Data integrity\n",
    "assert all(isinstance(r, str) for r in pretrain_data), \"Pretraining data not all strings.\"\n",
    "assert all('instruction' in s and 'response' in s for s in finetune_data), \"Fine-tuning data missing keys.\"\n",
    "print('Data integrity tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd2a7da",
   "metadata": {},
   "source": [
    "## 7. Quantize Model Weights\n",
    "- Quantize the trained model weights to 4-bit precision for edge device deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize model weights to 4-bit (demo: use torch.int8 for illustration)\n",
    "quantized_path = '/content/drive/MyDrive/GourmetGPT/model_state_dict_quantized.pth'\n",
    "quantized_model = model\n",
    "for param in quantized_model.parameters():\n",
    "    param.data = param.data.to(torch.int8)  # For demo; use real quantization for production\n",
    "\n",
    "torch.save(quantized_model.state_dict(), quantized_path)\n",
    "print(f'Quantized model weights saved to {quantized_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f1009",
   "metadata": {},
   "source": [
    "## 8. Offline Inference Test\n",
    "- Load exported artifacts and run offline recipe generation tests to validate inference on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline inference test (demo)\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "def generate_recipe(prompt, tokenizer, model, max_length=256):\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt')['input_ids']\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "        # For demo, just decode input (replace with real generation logic)\n",
    "        return tokenizer.decode(input_ids[0])\n",
    "\n",
    "sample_prompt = \"Generate a vegetarian pizza recipe.\"\n",
    "recipe = generate_recipe(sample_prompt, tokenizer, model)\n",
    "print('Generated Recipe:', recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7860a",
   "metadata": {},
   "source": [
    "## 9. Zip and Download Artifacts\n",
    "- Create a zip archive of all model artifacts and provide a cell to download them for handover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4332e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip and download all model artifacts\n",
    "import zipfile\n",
    "zip_path = '/content/drive/MyDrive/GourmetGPT/gourmetgpt_artifacts.zip'\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    zipf.write(model_path, arcname='model_state_dict.pth')\n",
    "    zipf.write(tokenizer_path, arcname='tokenizer.json')\n",
    "    zipf.write(config_path, arcname='config.json')\n",
    "    zipf.write(quantized_path, arcname='model_state_dict_quantized.pth')\n",
    "print(f'Artifacts zipped at {zip_path}')\n",
    "\n",
    "from google.colab import files\n",
    "files.download(zip_path)\n",
    "print('Download initiated for model artifacts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05537707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total number of trainable parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17b8a3",
   "metadata": {},
   "source": [
    "## 10. Training Metrics and Visualization for IEEE Reporting\n",
    "- Track and visualize training loss and perplexity\n",
    "- Example plots for inclusion in IEEE papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea112563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track and visualize training loss and perplexity for IEEE reporting\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "losses = []  # Collect loss values during training\n",
    "# Example: Append loss.item() inside your training loop\n",
    "# losses.append(loss.item())\n",
    "if losses:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(losses, label='Training Loss')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    print(f'Final Perplexity: {perplexity:.2f}')\n",
    "else:\n",
    "    print('No loss data collected. Please append loss values during training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb1c7a7",
   "metadata": {},
   "source": [
    "## 11. Additional Metrics for Constitution Compliance\n",
    "- Ingredient F1 Score\n",
    "- Human Evaluation\n",
    "- Model Footprint (parameters, memory)\n",
    "- Latency (inference time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingredient F1 Score (example implementation)\n",
    "def ingredient_f1(predicted, reference):\n",
    "    pred_set = set(predicted.lower().split(','))\n",
    "    ref_set = set(reference.lower().split(','))\n",
    "    tp = len(pred_set & ref_set)\n",
    "    fp = len(pred_set - ref_set)\n",
    "    fn = len(ref_set - pred_set)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "predicted_ingredients = \"egg, spinach, tomato, cheese\"\n",
    "reference_ingredients = \"egg, tomato, cheese, onion\"\n",
    "f1_score = ingredient_f1(predicted_ingredients, reference_ingredients)\n",
    "print(f'Ingredient F1 Score: {f1_score:.2f}')\n",
    "\n",
    "# Human Evaluation (manual entry example)\n",
    "human_scores = [4, 5, 3, 4]  # Example: ratings from 1-5\n",
    "avg_human_score = sum(human_scores) / len(human_scores)\n",
    "print(f'Average Human Evaluation Score: {avg_human_score:.2f}')\n",
    "\n",
    "# Model Footprint (parameters, memory)\n",
    "import torch\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "model_size_mb = sum(p.element_size() * p.nelement() for p in model.parameters()) / (1024 ** 2)\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Model size (MB): {model_size_mb:.2f}')\n",
    "\n",
    "# Latency (inference time)\n",
    "import time\n",
    "prompt = \"Healthy breakfast recipe\"\n",
    "input_ids = tokenizer(prompt, return_tensors='pt')['input_ids']\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "latency = time.time() - start_time\n",
    "print(f'Inference latency: {latency*1000:.2f} ms')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
