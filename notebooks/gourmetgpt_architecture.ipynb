{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer setup: load tokenizer and ensure [PAD] token exists\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "# Add pad token if missing and set it\n",
    "if tokenizer.pad_token is None or '[PAD]' not in tokenizer.get_vocab():\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.pad_token = '[PAD]'\n",
    "    print(\"Added and set [PAD] token as tokenizer.pad_token.\")\n",
    "else:\n",
    "    print(\"[PAD] token already present in tokenizer.\")\n",
    "# Expose vocab size for downstream model creation\n",
    "vocab_size = len(tokenizer)\n",
    "print(f'Tokenizer vocab size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility helpers to create tensors on the same device as the model\n",
    "import torch\n",
    "def to_model_device(tensor):\n",
    "    \"\"\"Move a tensor to the device where the model parameters live.\"\"\"\n",
    "    try:\n",
    "        device = next(model.parameters()).device\n",
    "    except Exception:\n",
    "        device = torch.device('cpu')\n",
    "    return tensor.to(device)\n",
    "\n",
    "def randint_on_model_device(low, high, size, dtype=torch.long):\n",
    "    \"\"\"Create a random integer tensor on the model device.\"\"\"\n",
    "    try:\n",
    "        device = next(model.parameters()).device\n",
    "    except Exception:\n",
    "        device = torch.device('cpu')\n",
    "    return torch.randint(low, high, size, dtype=dtype, device=device)\n",
    "\n",
    "print('Model device helpers installed (to_model_device, randint_on_model_device)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d436ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate pad-token check removed; consolidated at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71149fd6",
   "metadata": {},
   "source": [
    "# GourmetGPT: Recipe Generation Model Development\n",
    "\n",
    "This notebook implements, trains, evaluates, and exports the GourmetGPT model in accordance with the project constitution and specification.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Import Dependencies and Set Up Environment\n",
    "- Install and import required libraries (PyTorch, transformers, etc.)\n",
    "- Set random seeds for reproducibility\n",
    "- Configure Google Drive integration for artifact storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74571c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import required libraries\n",
    "!pip install torch transformers --quiet\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from google.colab import drive\n",
    "\n",
    "def set_seed(seed=123):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(123)\n",
    "h\n",
    "# Ensure CUDA is available and set global device (Colab: Runtime -> Change runtime type -> GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type != 'cuda':\n",
    "    raise RuntimeError(\"CUDA device not available. Please enable GPU runtime in Colab: Runtime -> Change runtime type -> GPU.\")\n",
    "print('Using device:', device)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "print('Google Drive mounted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e01ee",
   "metadata": {},
   "source": [
    "## 2. Load and Validate Datasets\n",
    "- Load pretraining dataset from Google Drive (`[BOS]... [EOS]` format)\n",
    "- Load fine-tuning dataset from Google Drive (no `[BOS]`/`[EOS]` tokens in `response`)\n",
    "- Validate schema and completeness\n",
    "- Perform automated checks as per constitution.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dbf4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretraining dataset (.txt)\n",
    "pretrain_path = '/content/drive/MyDrive/PhD/GourmetGPT/Dataset/structured_recipes_pretrain.txt'\n",
    "with open(pretrain_path, 'r', encoding='utf-8') as f:\n",
    "    pretrain_data = f.read().split('[EOS]')\n",
    "pretrain_data = [r.strip() for r in pretrain_data if r.strip()]\n",
    "print(f'Loaded {len(pretrain_data)} pretraining recipes.')\n",
    "\n",
    "# Validate pretraining schema\n",
    "for i, recipe in enumerate(pretrain_data[:3]):\n",
    "    assert '[BOS]' in recipe and 'Title:' in recipe and 'Ingredients:' in recipe and 'Instructions:' in recipe, f\"Schema error in recipe {i}\"\n",
    "print('Pretraining dataset schema validated.')\n",
    "\n",
    "# Load fine-tuning dataset (.jsonl)\n",
    "import json\n",
    "finetune_path = '/content/drive/MyDrive/PhD/GourmetGPT/Dataset/structured_recipes_finetune.jsonl'\n",
    "finetune_data = []\n",
    "with open(finetune_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        finetune_data.append(obj)\n",
    "print(f'Loaded {len(finetune_data)} fine-tuning samples.')\n",
    "\n",
    "# Validate fine-tuning schema (no [BOS]/[EOS] required)\n",
    "for i, sample in enumerate(finetune_data[:3]):\n",
    "    assert 'instruction' in sample and 'response' in sample, f\"Schema error in sample {i}\"\n",
    "    assert 'Title:' in sample['response'] and 'Ingredients:' in sample['response'] and 'Instructions:' in sample['response'], f\"Response schema error in sample {i}\"\n",
    "print('Fine-tuning dataset schema validated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268a14b",
   "metadata": {},
   "source": [
    "## 3. Design and Implement GPT Architecture\n",
    "- Define the GPT model architecture using PyTorch\n",
    "- Configure tokenizer and model configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07996ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GPT model architecture (simplified, for demonstration)\n",
    "import torch.nn as nn\n",
    "\n",
    "# vocab_size is derived from the tokenizer (created earlier)\n",
    "print(f'Using tokenizer vocab_size = {vocab_size}')\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x, x)\n",
    "        return self.fc_out(x)\n",
    "\n",
    "# Example config (now derived from tokenizer)\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "model = GPTModel(vocab_size, embed_dim, num_heads, num_layers)\n",
    "print(model)\n",
    "\n",
    "# If tokenizer is changed (e.g., added special tokens) after model creation,\n",
    "# resize embedding and output layers to match new vocab size while preserving weights where possible.\n",
    "new_vocab = len(tokenizer)\n",
    "if new_vocab != model.embedding.num_embeddings:\n",
    "    old_embed = model.embedding\n",
    "    new_embed = nn.Embedding(new_vocab, embed_dim)\n",
    "    # copy weights for overlapping indices\n",
    "    num_to_copy = min(old_embed.num_embeddings, new_embed.num_embeddings)\n",
    "    new_embed.weight.data[:num_to_copy] = old_embed.weight.data[:num_to_copy].clone()\n",
    "    model.embedding = new_embed\n",
    "    # adjust output layer\n",
    "    model.fc_out = nn.Linear(embed_dim, new_vocab)\n",
    "    print(f'Resized embedding and output layer to new vocab size: {new_vocab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45aaf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-flight tests: verify tokenizer, embeddings, and a dummy forward pass\n",
    "print('Running pre-flight checks...')\n",
    "# 1) pad token present and set\n",
    "assert tokenizer.pad_token is not None, 'tokenizer.pad_token is None'\n",
    "print(f'pad_token: {tokenizer.pad_token}')\n",
    "# 2) vocab vs embeddings\n",
    "assert vocab_size == model.embedding.num_embeddings, f'vocab_size ({vocab_size}) != embedding.num_embeddings ({model.embedding.num_embeddings})'\n",
    "print('vocab_size matches model.embedding.num_embeddings')\n",
    "# 3) dummy batched forward pass (small shapes to validate indices)\n",
    "import torch\n",
    "# ensure inputs are created on the same device as the model\n",
    "device = next(model.parameters()).device\n",
    "batch = torch.randint(0, vocab_size, (2, 8), device=device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(batch)\n",
    "assert out.shape == (2, 8, vocab_size), f'Unexpected output shape: {out.shape}'\n",
    "print('Dummy forward pass success:', out.shape)\n",
    "print('Pre-flight checks passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8633aab",
   "metadata": {},
   "source": [
    "## 4. Train Model and Save Checkpoints\n",
    "- Train the model on the loaded datasets\n",
    "- Periodically save checkpoints to Google Drive\n",
    "- Log training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79469e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized training loop for A100 GPU with mixed precision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# Use the global `device` set earlier (must be CUDA)\n",
    "model = model.to(device)\n",
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        return tokens['input_ids'].squeeze(0)\n",
    "train_dataset = RecipeDataset(pretrain_data, tokenizer)\n",
    "# Increase batch size for A100 (try 32, adjust if OOM)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "model.train()\n",
    "for epoch in range(1):  # Demo: 1 epoch\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(batch)\n",
    "            loss = nn.CrossEntropyLoss()(output.view(-1, vocab_size), batch.view(-1))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        # Save checkpoint every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            torch.save(model.state_dict(), '/content/drive/MyDrive/PhD/GourmetGPT/model_state_dict.pth')\n",
    "print('Training complete. Final checkpoint saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b2bf7",
   "metadata": {},
   "source": [
    "## 5. Export Model Artifacts\n",
    "- Export trained model state_dict, tokenizer, and config files as artifacts for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model artifacts\n",
    "model_path = '/content/drive/MyDrive/PhD/GourmetGPT/model_state_dict.pth'\n",
    "tokenizer_path = '/content/drive/MyDrive/PhD/GourmetGPT/tokenizer.json'\n",
    "config_path = '/content/drive/MyDrive/PhD/GourmetGPT/config.json'\n",
    "\n",
    "# Save model state_dict\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f'Model weights saved to {model_path}')\n",
    "\n",
    "# Save tokenizer\n",
    "if hasattr(tokenizer, 'save_pretrained'):\n",
    "    tokenizer.save_pretrained('/content/drive/MyDrive/PhD/GourmetGPT/')\n",
    "    print(f'Tokenizer saved to {tokenizer_path}')\n",
    "else:\n",
    "    import json\n",
    "    with open(tokenizer_path, 'w') as f:\n",
    "        json.dump(tokenizer.__dict__, f)\n",
    "    print(f'Tokenizer config saved to {tokenizer_path}')\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embed_dim': embed_dim,\n",
    "    'num_heads': num_heads,\n",
    "    'num_layers': num_layers\n",
    "}\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f)\n",
    "print(f'Model config saved to {config_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712aecc0",
   "metadata": {},
   "source": [
    "## 6. Unit Tests for Model and Data\n",
    "- Implement unit tests in notebook cells to verify model components and data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test: Model output shape\n",
    "# create sample input on same device as model to avoid device mismatch\n",
    "device = next(model.parameters()).device\n",
    "sample_input = torch.randint(0, vocab_size, (2, 256), device=device)  # batch_size=2, seq_len=256\n",
    "output = model(sample_input)\n",
    "assert output.shape == (2, 256, vocab_size), f\"Unexpected output shape: {output.shape}\"\n",
    "print('Model output shape test passed.')\n",
    "\n",
    "# Unit test: Data integrity\n",
    "assert all(isinstance(r, str) for r in pretrain_data), \"Pretraining data not all strings.\"\n",
    "assert all('instruction' in s and 'response' in s for s in finetune_data), \"Fine-tuning data missing keys.\"\n",
    "print('Data integrity tests passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd2a7da",
   "metadata": {},
   "source": [
    "## 7. Quantize Model Weights\n",
    "- Quantize the trained model weights to 4-bit precision for edge device deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Quantize Model Weights (commented out for now)\n",
    "# We keep this block as a reference. Disabled to avoid interference\n",
    "# with current inference/debugging while device/index issues are resolved.\n",
    "#\n",
    "# Example quantization (COMMENTED):\n",
    "# import torch\n",
    "# # Placeholder quantization library import\n",
    "# # from quant_lib import quantize_model_int4\n",
    "#\n",
    "# def quantize_and_save(model, path):\n",
    "#     \"\"\"Quantize model weights to int4 and save to `path`.\n",
    "#     NOTE: Adjust to the quantization library and device requirements.\n",
    "#     \"\"\"\n",
    "#     # Many quant libs expect model to be on CPU\n",
    "#     model_cpu = model.cpu()\n",
    "#     # qmodel = quantize_model_int4(model_cpu)\n",
    "#     # torch.save(qmodel.state_dict(), path)\n",
    "#     # print(f\"Quantized model saved to {path}\")\n",
    "#\n",
    "# # Example usage (disabled):\n",
    "# # quantize_and_save(model, '/content/drive/MyDrive/PhD/GourmetGPT/model_state_dict_quantized.pth')\n",
    "#\n",
    "# End of commented quantization block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f1009",
   "metadata": {},
   "source": [
    "## 8. Offline Inference Test\n",
    "- Load exported artifacts and run offline recipe generation tests to validate inference on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section: Offline inference test (device-safe)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Choose device (CUDA if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load state dict safely (map to CPU first, then move model to device)\n",
    "state = torch.load(model_path, map_location='cpu')\n",
    "model.load_state_dict(state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_recipe(prompt, tokenizer, model, max_length=256, temperature=1.0, top_k=50):\n",
    "    \"\"\"Device-safe simple autoregressive generator.\n",
    "    Returns decoded text (skip_special_tokens=True).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    # Tokenize and move to model device\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids'].to(device, dtype=torch.long)\n",
    "\n",
    "    # Determine eos token id if available\n",
    "    eos_id = getattr(tokenizer, 'eos_token_id', None)\n",
    "    if eos_id is None and tokenizer.eos_token:\n",
    "        eos_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_ids)\n",
    "            # Support different model return types\n",
    "            if isinstance(outputs, (list, tuple)):\n",
    "                logits = outputs[0]\n",
    "            elif hasattr(outputs, 'logits'):\n",
    "                logits = outputs.logits\n",
    "            else:\n",
    "                logits = outputs\n",
    "\n",
    "            next_logits = logits[:, -1, :]\n",
    "\n",
    "            # Top-k filtering\n",
    "            if top_k and top_k > 0:\n",
    "                values, indices = torch.topk(next_logits, top_k, dim=-1)\n",
    "                probs = torch.zeros_like(next_logits).to(device)\n",
    "                probs.scatter_(1, indices, F.softmax(values / max(temperature, 1e-8), dim=-1))\n",
    "            else:\n",
    "                probs = F.softmax(next_logits / max(temperature, 1e-8), dim=-1)\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            next_token = next_token.to(device, dtype=torch.long)\n",
    "\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "            if eos_id is not None and next_token.item() == eos_id:\n",
    "                break\n",
    "\n",
    "    generated = input_ids[0].cpu().tolist()\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "sample_prompt = \"Generate a vegetarian pizza recipe.\"\n",
    "recipe = generate_recipe(sample_prompt, tokenizer, model)\n",
    "print('Generated Recipe:', recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7860a",
   "metadata": {},
   "source": [
    "## 9. Zip and Download Artifacts\n",
    "- Create a zip archive of all model artifacts and provide a cell to download them for handover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4332e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Safe zip & download artifacts\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration: ensure these variables exist in your notebook scope ---\n",
    "# Expected variables (set earlier in the notebook):\n",
    "#   - zip_dir (folder to save the zip to)\n",
    "#   - model_path (path to model_state_dict.pth)\n",
    "#   - tokenizer_path (path to tokenizer.json or folder containing tokenizer files)\n",
    "#   - config_path (path to config.json)\n",
    "#   - quantized_path (optional, may be None)\n",
    "\n",
    "# Normalize vars from globals\n",
    "def get_var(name, default=None):\n",
    "    return globals().get(name, default)\n",
    "\n",
    "zip_dir = Path(get_var('zip_dir', None) or os.environ.get('ZIP_DIR') or '')\n",
    "model_path = get_var('model_path', None)\n",
    "tokenizer_path = get_var('tokenizer_path', None)\n",
    "config_path = get_var('config_path', None)\n",
    "quantized_path = get_var('quantized_path', None)\n",
    "\n",
    "if not zip_dir:\n",
    "    raise RuntimeError(\n",
    ")\n",
    "\n",
    "zip_dir = Path(zip_dir)\n",
    "zip_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Provide sensible defaults if some paths are None\n",
    "if model_path is None:\n",
    "    model_path = str(zip_dir / 'model_state_dict.pth')\n",
    "if tokenizer_path is None:\n",
    "    tokenizer_path = str(zip_dir / 'tokenizer.json')\n",
    "if config_path is None:\n",
    "    config_path = str(zip_dir / 'config.json')\n",
    "if quantized_path is None:\n",
    "    quantized_path = str(zip_dir / 'model_state_dict_quantized.pth')\n",
    "\n",
    "model_path = Path(model_path)\n",
    "tokenizer_path = Path(tokenizer_path)\n",
    "config_path = Path(config_path)\n",
    "quantized_path = Path(quantized_path)\n",
    "\n",
    "# Try to save missing artifacts from in-memory objects, if available\n",
    "try:\n",
    "    import torch\n",
    "    if 'model' in globals() and not model_path.exists():\n",
    "        try:\n",
    "            torch.save(globals()['model'].state_dict(), model_path)\n",
    "            print(f'Saved model state to {model_path}')\n",
    "        except Exception as e:\n",
    "            print(f'Could not save model state: {e}')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Save tokenizer if tokenizer object exists and tokenizer file missing\n",
    "try:\n",
    "    tokenizer = globals().get('tokenizer', None)\n",
    "    if tokenizer is not None and not tokenizer_path.exists():\n",
    "        if hasattr(tokenizer, \"save_pretrained\"):\n",
    "            try:\n",
    "                save_dir = zip_dir / 'tokenizer_files'\n",
    "                save_dir.mkdir(parents=True, exist_ok=True)\n",
    "                tokenizer.save_pretrained(save_dir)\n",
    "                if (save_dir / 'tokenizer.json').exists():\n",
    "                    tokenizer_path = save_dir / 'tokenizer.json'\n",
    "                else:\n",
    "                    tokenizer_path = save_dir\n",
    "                print(f'Saved tokenizer to {save_dir}')\n",
    "            except Exception as e:\n",
    "                print(f'Tokenizer save_pretrained failed: {e}')\n",
    "        else:\n",
    "            try:\n",
    "                if hasattr(tokenizer, \"to_json\") or hasattr(tokenizer, \"to_str\"):\n",
    "                    data = tokenizer.to_json() if hasattr(tokenizer, \"to_json\") else tokenizer.to_str()\n",
    "                    tokenizer_path.write_text(data, encoding='utf-8')\n",
    "                    print(f'Wrote tokenizer JSON to {tokenizer_path}')\n",
    "            except Exception as e:\n",
    "                print(f'Could not serialize tokenizer: {e}')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Required artifacts (must exist)\n",
    "required = {\n",
    "    'model_state_dict.pth': model_path,\n",
    "    'config.json': config_path,\n",
    "}\n",
    "\n",
    "tokenizer_is_dir = tokenizer_path.is_dir()\n",
    "if tokenizer_is_dir:\n",
    "    tokenizer_entries = [(str(p.relative_to(tokenizer_path.parent)), p) for p in tokenizer_path.rglob('*') if p.is_file()]\n",
    "else:\n",
    "    required['tokenizer.json'] = tokenizer_path\n",
    "\n",
    "optional = {\n",
    "    'model_state_dict_quantized.pth': quantized_path if quantized_path.exists() else None\n",
    "}\n",
    "\n",
    "missing_required = [name for name, p in required.items() if not Path(p).exists()]\n",
    "if missing_required:\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing required artifact(s): \" + \n",
    "        \n",
    "        \n",
    "        \n",
    "    )\n",
    "\n",
    "# Build list of archive entries\n",
    "to_zip = []\n",
    "for arcname, path in required.items():\n",
    "    p = Path(path)\n",
    "    to_zip.append((arcname, p))\n",
    "\n",
    "if tokenizer_is_dir:\n",
    "    for arc_rel, p in tokenizer_entries:\n",
    "        arcname = f\"tokenizer_files/{Path(arc_rel).name}\"\n",
    "        to_zip.append((arcname, p))\n",
    "elif 'tokenizer.json' in required and Path(required['tokenizer.json']).exists():\n",
    "    to_zip.append(('tokenizer.json', Path(required['tokenizer.json'])))\n",
    "\n",
    "if optional.get('model_state_dict_quantized.pth'):\n",
    "    to_zip.append(('model_state_dict_quantized.pth', Path(optional['model_state_dict_quantized.pth'])))\n",
    "\n",
    "zip_path = zip_dir / \"gourmetgpt_artifacts.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "    for arcname, filepath in to_zip:\n",
    "        try:\n",
    "            zf.write(filepath, arcname=arcname)\n",
    "            print(f\"Added {filepath} as {arcname}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not add {filepath}: {e}\")\n",
    "\n",
    "print(f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Download the zip manually from your Drive: {zip_path}\")    print(f\"Colab download not available or failed: {e}\")except Exception as e:    colab_files.download(str(zip_path))    print(\"Attempting to download zip via Colab (may be blocked for large files)...\")    from google.colab import files as colab_filestry:# Try to download in Colab (best-effort)Created zip: {zip_path} ({zip_path.stat().st_size / (1024*1024):.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05537707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total number of trainable parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17b8a3",
   "metadata": {},
   "source": [
    "## 10. Training Metrics and Visualization for IEEE Reporting\n",
    "- Track and visualize training loss and perplexity\n",
    "- Example plots for inclusion in IEEE papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea112563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track and visualize training loss and perplexity for IEEE reporting\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "losses = []  # Collect loss values during training\n",
    "# Example: Append loss.item() inside your training loop\n",
    "# losses.append(loss.item())\n",
    "if losses:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(losses, label='Training Loss')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    print(f'Final Perplexity: {perplexity:.2f}')\n",
    "else:\n",
    "    print('No loss data collected. Please append loss values during training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb1c7a7",
   "metadata": {},
   "source": [
    "## 11. Additional Metrics for Constitution Compliance\n",
    "- Ingredient F1 Score\n",
    "- Human Evaluation\n",
    "- Model Footprint (parameters, memory)\n",
    "- Latency (inference time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingredient F1 Score (example implementation)\n",
    "def ingredient_f1(predicted, reference):\n",
    "    pred_set = set(predicted.lower().split(','))\n",
    "    ref_set = set(reference.lower().split(','))\n",
    "    tp = len(pred_set & ref_set)\n",
    "    fp = len(pred_set - ref_set)\n",
    "    fn = len(ref_set - pred_set)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "predicted_ingredients = \"egg, spinach, tomato, cheese\"\n",
    "reference_ingredients = \"egg, tomato, cheese, onion\"\n",
    "f1_score = ingredient_f1(predicted_ingredients, reference_ingredients)\n",
    "print(f'Ingredient F1 Score: {f1_score:.2f}')\n",
    "\n",
    "# Human Evaluation (manual entry example)\n",
    "human_scores = [4, 5, 3, 4]  # Example: ratings from 1-5\n",
    "avg_human_score = sum(human_scores) / len(human_scores)\n",
    "print(f'Average Human Evaluation Score: {avg_human_score:.2f}')\n",
    "\n",
    "# Model Footprint (parameters, memory)\n",
    "import torch\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "model_size_mb = sum(p.element_size() * p.nelement() for p in model.parameters()) / (1024 ** 2)\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Model size (MB): {model_size_mb:.2f}')\n",
    "\n",
    "# Latency (inference time)\n",
    "import time\n",
    "prompt = \"Healthy breakfast recipe\"\n",
    "input_ids = tokenizer(prompt, return_tensors='pt')['input_ids']\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "latency = time.time() - start_time\n",
    "print(f'Inference latency: {latency*1000:.2f} ms')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
