{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer setup: load tokenizer and ensure [PAD] token exists\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "# Add pad token if missing and set it\n",
    "if tokenizer.pad_token is None or '[PAD]' not in tokenizer.get_vocab():\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.pad_token = '[PAD]'\n",
    "    print(\"Added and set [PAD] token as tokenizer.pad_token.\")\n",
    "else:\n",
    "    print(\"[PAD] token already present in tokenizer.\")\n",
    "# Expose vocab size for downstream model creation\n",
    "vocab_size = len(tokenizer)\n",
    "print(f'Tokenizer vocab size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility helpers to create tensors on the same device as the model\n",
    "import torch\n",
    "def to_model_device(tensor):\n",
    "    \"\"\"Move a tensor to the device where the model parameters live.\"\"\"\n",
    "    try:\n",
    "        device = next(model.parameters()).device\n",
    "    except Exception:\n",
    "        # Require global `device` (must be CUDA). Fail hard if CUDA not available.\n",
    "        if 'device' in globals() and globals()['device'].type == 'cuda':\n",
    "            device = globals()['device']\n",
    "        else:\n",
    "            raise RuntimeError(\"CUDA device not available. Please enable GPU runtime in Colab (Runtime -> Change runtime type -> GPU) before running this notebook.\")\n",
    "    return tensor.to(device)\n",
    "\n",
    "def randint_on_model_device(low, high, size, dtype=torch.long):\n",
    "    \"\"\"Create a random integer tensor on the model device.\"\"\"\n",
    "    try:\n",
    "        device = next(model.parameters()).device\n",
    "    except Exception:\n",
    "        # Require global `device` (must be CUDA). Fail hard if CUDA not available.\n",
    "        if 'device' in globals() and globals()['device'].type == 'cuda':\n",
    "            device = globals()['device']\n",
    "        else:\n",
    "            raise RuntimeError(\"CUDA device not available. Please enable GPU runtime in Colab (Runtime -> Change runtime type -> GPU) before running this notebook.\")\n",
    "    return torch.randint(low, high, size, dtype=dtype, device=device)\n",
    "\n",
    "print('Model device helpers installed (to_model_device, randint_on_model_device)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d436ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate pad-token check removed; consolidated at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71149fd6",
   "metadata": {},
   "source": [
    "# GourmetGPT: Recipe Generation Model Development\n",
    "\n",
    "This notebook implements, trains, evaluates, and exports the GourmetGPT model in accordance with the project constitution and specification.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Import Dependencies and Set Up Environment\n",
    "- Install and import required libraries (PyTorch, transformers, etc.)\n",
    "- Set random seeds for reproducibility\n",
    "- Configure Google Drive integration for artifact storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74571c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import required libraries\n",
    "!pip install torch transformers --quiet\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from google.colab import drive\n",
    "\n",
    "def set_seed(seed=123):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(123)\n",
    "# Ensure CUDA is available and set global device (Colab: Runtime -> Change runtime type -> GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type != 'cuda':\n",
    "    raise RuntimeError(\"CUDA device not available. Please enable GPU runtime in Colab: Runtime -> Change runtime type -> GPU.\")\n",
    "print('Using device:', device)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "print('Google Drive mounted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e01ee",
   "metadata": {},
   "source": [
    "## 2. Load and Validate Datasets\n",
    "- Load pretraining dataset from Google Drive (`[BOS]... [EOS]` format)\n",
    "- Load fine-tuning dataset from Google Drive (no `[BOS]`/`[EOS]` tokens in `response`)\n",
    "- Validate schema and completeness\n",
    "- Perform automated checks as per constitution.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dbf4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretraining dataset (.txt)\n",
    "pretrain_path = '/content/drive/MyDrive/PhD/GourmetGPT/Dataset/structured_recipes_pretrain.txt'\n",
    "with open(pretrain_path, 'r', encoding='utf-8') as f:\n",
    "    pretrain_data = f.read().split('[EOS]')\n",
    "pretrain_data = [r.strip() for r in pretrain_data if r.strip()]\n",
    "print(f'Loaded {len(pretrain_data)} pretraining recipes.')\n",
    "\n",
    "# Validate pretraining schema\n",
    "for i, recipe in enumerate(pretrain_data[:3]):\n",
    "    assert '[BOS]' in recipe and 'Title:' in recipe and 'Ingredients:' in recipe and 'Instructions:' in recipe, f\"Schema error in recipe {i}\"\n",
    "print('Pretraining dataset schema validated.')\n",
    "\n",
    "# Load fine-tuning dataset (.jsonl)\n",
    "import json\n",
    "finetune_path = '/content/drive/MyDrive/PhD/GourmetGPT/Dataset/structured_recipes_finetune.jsonl'\n",
    "finetune_data = []\n",
    "with open(finetune_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        finetune_data.append(obj)\n",
    "print(f'Loaded {len(finetune_data)} fine-tuning samples.')\n",
    "\n",
    "# Validate fine-tuning schema (no [BOS]/[EOS] required)\n",
    "for i, sample in enumerate(finetune_data[:3]):\n",
    "    assert 'instruction' in sample and 'response' in sample, f\"Schema error in sample {i}\"\n",
    "    assert 'Title:' in sample['response'] and 'Ingredients:' in sample['response'] and 'Instructions:' in sample['response'], f\"Response schema error in sample {i}\"\n",
    "print('Fine-tuning dataset schema validated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7dc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingredient F1 Score (example implementation)\n",
    "def ingredient_f1(predicted, reference):\n",
    "    pred_set = set(predicted.lower().split(','))\n",
    "    ref_set = set(reference.lower().split(','))\n",
    "    tp = len(pred_set & ref_set)\n",
    "    fp = len(pred_set - ref_set)\n",
    "    fn = len(ref_set - pred_set)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "predicted_ingredients = \"egg, spinach, tomato, cheese\"\n",
    "reference_ingredients = \"egg, tomato, cheese, onion\"\n",
    "f1_score = ingredient_f1(predicted_ingredients, reference_ingredients)\n",
    "print(f'Ingredient F1 Score: {f1_score:.2f}')\n",
    "\n",
    "# Human Evaluation (manual entry example)\n",
    "human_scores = [4, 5, 3, 4]  # Example: ratings from 1-5\n",
    "avg_human_score = sum(human_scores) / len(human_scores)\n",
    "print(f'Average Human Evaluation Score: {avg_human_score:.2f}')\n",
    "\n",
    "# Model Footprint (parameters, memory)\n",
    "import torch\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "model_size_mb = sum(p.element_size() * p.nelement() for p in model.parameters()) / (1024 ** 2)\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Model size (MB): {model_size_mb:.2f}')\n",
    "\n",
    "# Latency (inference time) - robust GPU timing\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Ensure a CUDA device is available (not changing notebook policy)\n",
    "if 'device' not in globals() or globals()['device'].type != 'cuda':\n",
    "    raise RuntimeError(\"CUDA device not available. Please enable GPU runtime in Colab (Runtime -> Change runtime type -> GPU).\")\n",
    "device = globals()['device']\n",
    "\n",
    "# Ensure model is on device and in eval mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Configurable measurement parameters\n",
    "prompt = \"Generate a vegetarian pizza recipe.\"  # set a sensible default or pass in a variable\n",
    "batch_size = 1\n",
    "seq_len = 64             # representative sequence length\n",
    "repeat = 20              # measured iterations\n",
    "warmup = 5               # warm-up iterations to stabilize GPU\n",
    "\n",
    "# Tokenize and move inputs to device\n",
    "inputs = tokenizer(prompt, return_tensors='pt', truncation=True, padding='max_length', max_length=seq_len, add_special_tokens=False)\n",
    "input_ids = inputs['input_ids'].to(device, dtype=torch.long).repeat(batch_size, 1)\n",
    "\n",
    "# Warm-up runs (no timing)\n",
    "with torch.no_grad():\n",
    "    for _ in range(warmup):\n",
    "        _ = model(input_ids)\n",
    "\n",
    "# Measured runs with CUDA synchronization for accurate GPU timing\n",
    "latencies = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(repeat):\n",
    "        torch.cuda.synchronize()\n",
    "        t0 = time.time()\n",
    "        _ = model(input_ids)\n",
    "        torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "        latencies.append(t1 - t0)\n",
    "\n",
    "latencies = np.array(latencies)\n",
    "median_s = float(np.median(latencies))\n",
    "mean_s = float(np.mean(latencies))\n",
    "per_sequence_ms = median_s * 1000.0\n",
    "per_token_ms = per_sequence_ms / seq_len\n",
    "\n",
    "print(f\"Latency (median over {repeat} runs, after {warmup} warm-up): {per_sequence_ms:.2f} ms per sequence\")\n",
    "print(f\"Per-token (median): {per_token_ms:.3f} ms (seq_len={seq_len}, batch_size={batch_size})\")\n",
    "print(f\"Mean latency: {mean_s*1000.0:.2f} ms; runs: {repeat}; warmup: {warmup}\")\n",
    "\n",
    "# Return results programmatically if needed\n",
    "latency_results = {\n",
    "    'median_s': median_s,\n",
    "    'mean_s': mean_s,\n",
    "    'per_sequence_ms': per_sequence_ms,\n",
    "    'per_token_ms': per_token_ms,\n",
    "    'seq_len': seq_len,\n",
    "    'batch_size': batch_size,\n",
    "    'repeat': repeat,\n",
    "    'warmup': warmup,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c7f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79091b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f902c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbdff7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f549fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d684fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741a65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c282db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962806b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
